{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob1zP4GLBjK4",
        "outputId": "e934c614-e0c8-4c7f-b8ff-e7dbc2c857e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# install all dependencies\n",
        "!pip install pytube\n",
        "!pip install python-dotenv\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "!pip install openai\n",
        "\n",
        "!pip install --ignore-installed PyYAML\n",
        "!pip install langchain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import secrets\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "ASSEMBLYAI_API_SECRET = os.getenv(\"ASSEMBLYAI_API_SECRET\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**AssemblyAI Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AssemblyAI integration\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "assemblyAI_base_url = \"https://api.assemblyai.com/v2\"\n",
        "\n",
        "assemblyAI_headers = {\n",
        "  \"authorization\": ASSEMBLYAI_API_SECRET,\n",
        "}\n",
        "\n",
        "# upload file to assemblyAI\n",
        "def upload_file(filename):\n",
        "  with open(filename, \"rb\") as f:\n",
        "    upload_url = assemblyAI_base_url + \"/upload\"\n",
        "    response = requests.post(upload_url, headers=assemblyAI_headers, data=f)\n",
        "\n",
        "  upload_url = response.json()[\"upload_url\"]\n",
        "  return upload_url\n",
        "\n",
        "# transcribe the file\n",
        "def create_transcript(upload_url):\n",
        "  transcribe_url = assemblyAI_base_url + \"/transcript\"\n",
        "  data = {\n",
        "    \"audio_url\": upload_url\n",
        "  }\n",
        "  response = requests.post(transcribe_url, json=data, headers=assemblyAI_headers)\n",
        "\n",
        "  transcript_id = response.json()['id']\n",
        "  polling_url = transcribe_url + \"/\" + transcript_id\n",
        "\n",
        "  while True:\n",
        "    transcription_result = requests.get(polling_url, headers=assemblyAI_headers).json()\n",
        "\n",
        "    if transcription_result['status'] == 'completed':\n",
        "      result = transcription_result['text']\n",
        "      return result\n",
        "\n",
        "    elif transcription_result['status'] == 'error':\n",
        "      raise RuntimeError(f\"Transcription failed: {transcription_result['error']}\")\n",
        "\n",
        "    else:\n",
        "      time.sleep(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 0: Process the input file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "videos = []\n",
        "\n",
        "with open('videos.csv', mode='r') as csv_file:\n",
        "  csv_reader = csv.DictReader(csv_file)\n",
        "  line_count = 0\n",
        "  for row in csv_reader:\n",
        "    if line_count == 0:\n",
        "      line_count += 1\n",
        "    videos.append(row)\n",
        "    line_count += 1\n",
        "print(videos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1: Download and transcribe videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "61wh2E5KM5d1",
        "outputId": "9c72adaa-6fb7-4c04-e884-6ff0b4d5e55b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from pytube import YouTube\n",
        "\n",
        "def transcribe_video(video_url):\n",
        "  # download the video\n",
        "  yt = YouTube(video_url)\n",
        "  audio = yt.streams.filter(only_audio=True).first()\n",
        "  file_id = yt.video_id\n",
        "  video_file = './videos/' + file_id + '.mp3'\n",
        "  text_file = './transcripts/' + file_id + '.txt'\n",
        "  \n",
        "  if not os.path.exists(video_file):\n",
        "    print(f'Downloading video id {file_id}')\n",
        "    out_file = audio.download(filename=file_id)\n",
        "    os.rename(out_file, video_file)\n",
        "\n",
        "  if not os.path.exists(text_file):\n",
        "    print(f'Uploading video id {file_id} to assemblyAI')\n",
        "    # upload the file to assemblyAI\n",
        "    upload_url = upload_file(video_file)\n",
        "\n",
        "    # transcribe the file\n",
        "    print(f'Generating transcript of video id {file_id}')\n",
        "    transcript = create_transcript(upload_url)\n",
        "    f = open(text_file, 'w')\n",
        "    f.write(transcript)\n",
        "    f.close()\n",
        "\n",
        "for video in videos:\n",
        "  transcribe_video(video['link'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2a: Create an abstractive summary of the transcript using Googleâ€™s PEGASUS model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load tokenizer\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model\n",
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def generate_summary(text):\n",
        "  tokens = tokenizer(text, truncation=True, padding='longest', return_tensors='pt')\n",
        "  summary_tokens = model.generate(**tokens)\n",
        "  summary = tokenizer.decode(summary_tokens[0])\n",
        "  return summary\n",
        "\n",
        "for video in videos:\n",
        "  # read the video transcript\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  transcript_file = open('./transcripts/' + video_id + '.txt', 'r')\n",
        "  video_transcript = transcript_file.read()\n",
        "\n",
        "  summary_file = './summaries/pegasus-' + video_id + '.txt'\n",
        "  if not os.path.exists(summary_file):\n",
        "    print(f'Pegasus: Generating summary of video id {video_id}')\n",
        "\n",
        "    # run the summary generation workflow\n",
        "    summary = generate_summary(video_transcript)\n",
        "\n",
        "    # save summary to file\n",
        "    f = open(summary_file, 'w')\n",
        "    f.write(summary)\n",
        "    f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# clean up the summary data generated by Pegasus\n",
        "import re\n",
        "CLEANR = re.compile('<.*?>') \n",
        "\n",
        "def cleanhtml(raw_html):\n",
        "  cleantext = re.sub(CLEANR, '', raw_html)\n",
        "  return cleantext\n",
        "\n",
        "for video in videos:\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  summary_file = open('./summaries/pegasus-' + video_id + '.txt', 'r')\n",
        "  summary = summary_file.read()\n",
        "  summary = cleanhtml(summary)\n",
        "  clean_summary_file = open('./summaries/pegasus-' + video_id + '-clean.txt', 'w')\n",
        "  clean_summary_file.write(summary)\n",
        "  clean_summary_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2b: Create an abstractive summary of the transcript using OpenAI's GPT-3 model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# split the video transcript into chunks of 2048 characters\n",
        "def split_text(text):\n",
        "  max_chunk_size = 2048\n",
        "  chunks = []\n",
        "  current_chunk = ''\n",
        "  for sentence in text.split('.'):\n",
        "    if len(current_chunk) + len(sentence) < max_chunk_size:\n",
        "      current_chunk += sentence + '.'\n",
        "    else:\n",
        "      chunks.append(current_chunk.strip())\n",
        "      current_chunk = sentence + '.'\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk.strip())\n",
        "  return chunks\n",
        "\n",
        "# generate summary using openai\n",
        "def generate_summary(text):\n",
        "  input_chunks = split_text(text)\n",
        "  output_chunks = []\n",
        "  for chunk in input_chunks:\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"davinci\",\n",
        "      prompt=(f\"Please summarize the following text: {chunk}\\n\"),\n",
        "      temperature=0.5,\n",
        "      max_tokens=1024,\n",
        "    )\n",
        "    summary = response.choices[0].text.strip()\n",
        "    output_chunks.append(summary)\n",
        "  return ''.join(output_chunks)\n",
        "\n",
        "\n",
        "for video in videos:\n",
        "  # read the video transcript\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  transcript_file = open('./transcripts/' + video_id + '.txt', 'r')\n",
        "  video_transcript = transcript_file.read()\n",
        "\n",
        "  summary_file = './summaries/openai-' + video_id + '.txt'\n",
        "  if not os.path.exists(summary_file):\n",
        "    print(f'OpenAI: Generating summary of video id {video_id}')\n",
        "\n",
        "    # run the summary generation workflow\n",
        "    summary = generate_summary(video_transcript)\n",
        "\n",
        "    # save summary to file\n",
        "    f = open(summary_file, 'w')\n",
        "    f.write(summary)\n",
        "    f.close()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Create embeddings of the transcript summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import openai\n",
        "import csv\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "def create_embeddings(video_id, summary_path):\n",
        "  data_path = './data/' + summary_path + video_id + '.csv'\n",
        "  if not os.path.exists(data_path):\n",
        "    with open(data_path, 'w') as csv_file:\n",
        "      csv_writer = csv.writer(csv_file)\n",
        "\n",
        "      # write the header\n",
        "      csv_writer.writerow(['video_id', 'text', 'embedding'])\n",
        "\n",
        "      # write the data rows\n",
        "      with open('./summaries/' + summary_path + video_id + '.txt') as text_file:\n",
        "        summary = text_file.read()\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "          chunk_size = 2048,\n",
        "          chunk_overlap  = 20,\n",
        "        )\n",
        "        documents = text_splitter.create_documents([summary])\n",
        "\n",
        "        for text in documents:\n",
        "          embedding=openai.Embedding.create(model=\"text-embedding-ada-002\", input=text.page_content)\n",
        "          query_result=embedding['data'][0]['embedding']\n",
        "          csv_writer.writerow([video_id, text.page_content, query_result])\n",
        "\n",
        "# generate embeddings for the openai summaries\n",
        "summary_path='openai-'\n",
        "for video in videos:\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  print(f'OpenAI: Generating embeddings for video id {video_id}')\n",
        "  create_embeddings(video_id, summary_path)\n",
        "\n",
        "# generate embeddings for the pegasus summaries\n",
        "summary_path='pegasus-'\n",
        "for video in videos:\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  print(f'Pegasus: Generating embeddings for video id {video_id}')\n",
        "  create_embeddings(video_id, summary_path)\n",
        " "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_dataset(path):\n",
        "  embeddings = np.empty((0, 1536), float)\n",
        "  embeddings_df = pd.DataFrame()\n",
        "\n",
        "  for video in videos:\n",
        "    video_id = video['link'].split('=')[1]\n",
        "    # print(video_id)\n",
        "    datafile_path = './data/' + path + video_id + '.csv'\n",
        "    df = pd.read_csv(datafile_path)\n",
        "    df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n",
        "    matrix = np.vstack(df.embedding.values)\n",
        "\n",
        "    # append the matrix and df to the embeddings\n",
        "    embeddings = np.vstack((embeddings, matrix))\n",
        "    # print(embeddings.shape)\n",
        "    embeddings_df = embeddings_df.append(df)\n",
        "    # print(embeddings_df.shape)\n",
        "\n",
        "  return embeddings, embeddings_df\n",
        "\n",
        "def create_clusters(embeddings, embeddings_df):\n",
        "  kmeans = KMeans(n_clusters=5, init=\"k-means++\", random_state=42)\n",
        "  kmeans.fit(embeddings)\n",
        "  labels = kmeans.labels_\n",
        "  embeddings_df['cluster'] = labels\n",
        "  \n",
        "  return embeddings, embeddings_df\n",
        "\n",
        "def plot_clusters(path, embeddings, embeddings_df):\n",
        "  tsne = TSNE(n_components=2, perplexity=3, random_state=42, init=\"random\", learning_rate=200)\n",
        "  vis_dims2 = tsne.fit_transform(embeddings)\n",
        "\n",
        "  x_coords = [x for x, y in vis_dims2]\n",
        "  y_coords = [y for x, y in vis_dims2]\n",
        "\n",
        "  legend_map = {}\n",
        "\n",
        "  # plot the points\n",
        "  for category, color in enumerate([\"purple\", \"green\", \"red\", \"blue\", \"yellow\"]):\n",
        "    xs = np.array(x_coords)[np.where(embeddings_df['cluster'] == category)[0]]\n",
        "    ys = np.array(y_coords)[np.where(embeddings_df['cluster'] == category)[0]]\n",
        "    # print(category, color, xs.shape, ys.shape)\n",
        "    \n",
        "    # get the video ids for the cluster of points\n",
        "    video_ids = embeddings_df.loc[(embeddings_df['cluster'] == category), 'video_id'].values\n",
        "    video_ids = np.unique(video_ids)\n",
        "\n",
        "    # filter the videos by the video ids that belong to this cluster\n",
        "    filtered_videos = [v for v in videos if v['link'].split('=')[1] in (video_ids)]\n",
        "    categories = np.unique([video['category'] for video in filtered_videos])\n",
        "    # print(filtered_videos)\n",
        "    # print(categories)\n",
        "\n",
        "    # plot the points \n",
        "    plt.scatter(xs, ys, color=color, alpha=0.3, label=None)\n",
        "\n",
        "    # plot the legend\n",
        "    legend_map[color] = categories\n",
        "    \n",
        "  for color in legend_map:\n",
        "    plt.scatter([], [], c=color, alpha=0.3, label=legend_map[color])\n",
        "\n",
        "  plt.legend(loc=\"upper right\", scatterpoints=1, frameon=False, labelspacing=0.5)\n",
        "  plt.title(\"Clusters identified visualized in 2d using t-SNE\")\n",
        "  plt.savefig('./images/' + path + 'clustering.png') \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path='openai-'\n",
        "\n",
        "# create dataset\n",
        "embeddings, embeddings_df = create_dataset(path)\n",
        "\n",
        "# create clusters\n",
        "embeddings, embeddings_df = create_clusters(embeddings, embeddings_df)\n",
        "\n",
        "# create clusters dataframe\n",
        "clusters_df = embeddings_df[['video_id', 'cluster']]\n",
        "clusters_df_unique = clusters_df.drop_duplicates()\n",
        "\n",
        "# save clusters to file\n",
        "clusters_file = open('./clustering/' + path + 'clusters.csv', 'w')\n",
        "clusters_df_unique.to_csv(clusters_file, index=False)\n",
        "\n",
        "# plot clusters\n",
        "plot_clusters(path, embeddings, embeddings_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "path='pegasus-'\n",
        "\n",
        "# create dataset\n",
        "embeddings, embeddings_df = create_dataset(path)\n",
        "\n",
        "# create clusters\n",
        "embeddings, embeddings_df = create_clusters(embeddings, embeddings_df)\n",
        "\n",
        "# create clusters dataframe\n",
        "clusters_df = embeddings_df[['video_id', 'cluster']]\n",
        "clusters_df_unique = clusters_df.drop_duplicates()\n",
        "\n",
        "# save clusters to file\n",
        "clusters_file = open('./clustering/' + path + 'clusters.csv', 'w')\n",
        "clusters_df_unique.to_csv(clusters_file, index=False)\n",
        "\n",
        "# plot clusters\n",
        "plot_clusters(path, embeddings, embeddings_df)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 5: Classification**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "import csv\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "categories = ['finance', 'food', 'travel', 'career', 'productivity']\n",
        "\n",
        "# obtain embeddings of the categories\n",
        "categories_file = './data/categories.csv'\n",
        "if not os.path.exists(categories_file):\n",
        "  with open(categories_file, 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['category', 'embedding'])\n",
        "    for category in categories:\n",
        "      print(f'Generating embeddings for category {category}')\n",
        "      embedding=openai.Embedding.create(model=\"text-embedding-ada-002\", input=category)\n",
        "      query_result=embedding['data'][0]['embedding']\n",
        "      csv_writer.writerow([category, query_result])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy import spatial\n",
        "\n",
        "# classify videos into categories\n",
        "\n",
        "# read category embeddings\n",
        "category_df = pd.read_csv('./data/categories.csv')\n",
        "category_df[\"embedding\"] = category_df.embedding.apply(eval).apply(np.array)\n",
        "category_matrix = np.vstack(category_df.embedding.values)\n",
        "print('category matrix shape: ', category_matrix.shape)\n",
        "\n",
        "# nearest neighbors fit\n",
        "neighbors = NearestNeighbors(n_neighbors=1, metric=spatial.distance.cosine)\n",
        "neighbors.fit(category_matrix)\n",
        "\n",
        "paths = ['openai-', 'pegasus-']\n",
        "\n",
        "for path in paths:\n",
        "  # open a file to write the results\n",
        "  classification_file = './classification/' + path + 'classification.csv'\n",
        "  with open(classification_file, 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "    csv_writer.writerow(['video_id', 'category'])\n",
        "\n",
        "    # read data embeddings\n",
        "    for video in videos:\n",
        "      # define empty list of embeddings\n",
        "      embeddings = np.empty((0, 1536), float) \n",
        "\n",
        "      # get video id\n",
        "      video_id = video['link'].split('=')[1]\n",
        "      # print('video id: ', video_id)\n",
        "\n",
        "      # read data embeddings\n",
        "      datafile_path = './data/' + 'openai-' + video_id + '.csv'\n",
        "      df = pd.read_csv(datafile_path)\n",
        "      df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n",
        "      embeddings = np.vstack(df.embedding.values)\n",
        "      # print(embeddings.shape)\n",
        "      \n",
        "      # compute the centroid of summary embeddings\n",
        "      centroid = embeddings.mean(axis=0)\n",
        "      \n",
        "      # classify using KNN\n",
        "      index = neighbors.kneighbors([centroid], return_distance=False)[0, 0]\n",
        "      category = categories[index]\n",
        "      # print('category: ', category)\n",
        "\n",
        "      # write to file\n",
        "      csv_writer.writerow([video_id, category])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
