{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob1zP4GLBjK4",
        "outputId": "e934c614-e0c8-4c7f-b8ff-e7dbc2c857e4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# install all dependencies\n",
        "!pip install pytube\n",
        "!pip install python-dotenv\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "\n",
        "!pip install openai\n",
        "\n",
        "!pip install --ignore-installed PyYAML\n",
        "!pip install langchain\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import secrets\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "ASSEMBLYAI_API_SECRET = os.getenv(\"ASSEMBLYAI_API_SECRET\")\n",
        "\n",
        "print(\"OPENAI_API_KEY: \", OPENAI_API_KEY)\n",
        "print(\"ASSEMBLYAI_API_SECRET: \", ASSEMBLYAI_API_SECRET)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**AssemblyAI Helper functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AssemblyAI integration\n",
        "import requests\n",
        "import json\n",
        "import time\n",
        "\n",
        "assemblyAI_base_url = \"https://api.assemblyai.com/v2\"\n",
        "\n",
        "assemblyAI_headers = {\n",
        "  \"authorization\": ASSEMBLYAI_API_SECRET,\n",
        "}\n",
        "\n",
        "# upload file to assemblyAI\n",
        "def upload_file(filename):\n",
        "  with open(filename, \"rb\") as f:\n",
        "    upload_url = assemblyAI_base_url + \"/upload\"\n",
        "    response = requests.post(upload_url, headers=assemblyAI_headers, data=f)\n",
        "\n",
        "  upload_url = response.json()[\"upload_url\"]\n",
        "  return upload_url\n",
        "\n",
        "# transcribe the file\n",
        "def create_transcript(upload_url):\n",
        "  transcribe_url = assemblyAI_base_url + \"/transcript\"\n",
        "  data = {\n",
        "    \"audio_url\": upload_url\n",
        "  }\n",
        "  response = requests.post(transcribe_url, json=data, headers=assemblyAI_headers)\n",
        "\n",
        "  transcript_id = response.json()['id']\n",
        "  polling_url = transcribe_url + \"/\" + transcript_id\n",
        "\n",
        "  while True:\n",
        "    transcription_result = requests.get(polling_url, headers=assemblyAI_headers).json()\n",
        "\n",
        "    if transcription_result['status'] == 'completed':\n",
        "      result = transcription_result['text']\n",
        "      return result\n",
        "\n",
        "    elif transcription_result['status'] == 'error':\n",
        "      raise RuntimeError(f\"Transcription failed: {transcription_result['error']}\")\n",
        "\n",
        "    else:\n",
        "      time.sleep(3)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 0: Process the input file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "videos = []\n",
        "\n",
        "with open('videos.csv', mode='r') as csv_file:\n",
        "  csv_reader = csv.DictReader(csv_file)\n",
        "  line_count = 0\n",
        "  for row in csv_reader:\n",
        "    if line_count == 0:\n",
        "      line_count += 1\n",
        "    videos.append(row)\n",
        "    line_count += 1\n",
        "print(videos)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 1: Download and transcribe videos**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "61wh2E5KM5d1",
        "outputId": "9c72adaa-6fb7-4c04-e884-6ff0b4d5e55b"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "from pytube import YouTube\n",
        "\n",
        "def transcribe_video(video_url):\n",
        "  # download the video\n",
        "  yt = YouTube(video_url)\n",
        "  audio = yt.streams.filter(only_audio=True).first()\n",
        "  file_id = yt.video_id\n",
        "  video_file = './videos/' + file_id + '.mp3'\n",
        "  text_file = './transcripts/' + file_id + '.txt'\n",
        "  \n",
        "  if not os.path.exists(video_file):\n",
        "    print(f'Downloading video id {file_id}')\n",
        "    out_file = audio.download(filename=file_id)\n",
        "    os.rename(out_file, video_file)\n",
        "\n",
        "  if not os.path.exists(text_file):\n",
        "    print(f'Uploading video id {file_id} to assemblyAI')\n",
        "    # upload the file to assemblyAI\n",
        "    upload_url = upload_file(video_file)\n",
        "\n",
        "    # transcribe the file\n",
        "    print(f'Generating transcript of video id {file_id}')\n",
        "    transcript = create_transcript(upload_url)\n",
        "    f = open(text_file, 'w')\n",
        "    f.write(transcript)\n",
        "    f.close()\n",
        "\n",
        "for video in videos:\n",
        "  transcribe_video(video['link'])\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Read the video file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "transcript_file = open('./transcripts/' + video_id + '.txt', 'r')\n",
        "video_transcript = transcript_file.read()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2a: Create an abstractive summary of the transcript using Googleâ€™s PEGASUS model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load tokenizer\n",
        "tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load model\n",
        "model = PegasusForConditionalGeneration.from_pretrained('google/pegasus-xsum')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "tokens = tokenizer(video_transcript, truncation=True, padding='longest', return_tensors='pt')\n",
        "summary_tokens = model.generate(**tokens)\n",
        "summary = tokenizer.decode(summary_tokens[0])\n",
        "\n",
        "transcript_file = open('./summaries/pegasus-' + video_id + '.txt', 'w')\n",
        "transcript_file.write(summary)\n",
        "transcript_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 2b: Create an abstractive summary of the transcript using OpenAI's GPT-3 model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "# split the video transcript into chunks of 2048 characters\n",
        "def split_text(text):\n",
        "  max_chunk_size = 2048\n",
        "  chunks = []\n",
        "  current_chunk = ''\n",
        "  for sentence in text.split('.'):\n",
        "    if len(current_chunk) + len(sentence) < max_chunk_size:\n",
        "      current_chunk += sentence + '.'\n",
        "    else:\n",
        "      chunks.append(current_chunk.strip())\n",
        "      current_chunk = sentence + '.'\n",
        "  if current_chunk:\n",
        "    chunks.append(current_chunk.strip())\n",
        "  return chunks\n",
        "\n",
        "# generate summary using openai\n",
        "def generate_summary(text):\n",
        "  input_chunks = split_text(text)\n",
        "  output_chunks = []\n",
        "  for chunk in input_chunks:\n",
        "    response = openai.Completion.create(\n",
        "      engine=\"davinci\",\n",
        "      prompt=(f\"Please summarize the following text: {chunk}\\n\"),\n",
        "      temperature=0.5,\n",
        "      max_tokens=1024,\n",
        "    )\n",
        "    summary = response.choices[0].text.strip()\n",
        "    print('chunk: ', chunk)\n",
        "    print('summary: ', summary)\n",
        "    output_chunks.append(summary)\n",
        "  return ''.join(output_chunks)\n",
        "\n",
        "\n",
        "for video in videos:\n",
        "  # read the video transcript\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  transcript_file = open('./transcripts/' + video_id + '.txt', 'r')\n",
        "  video_transcript = transcript_file.read()\n",
        "\n",
        "  summary_file = './summaries/openai-' + video_id + '.txt'\n",
        "  if not os.path.exists(summary_file):\n",
        "    print(f'Generating summary of video id {video_id}')\n",
        "\n",
        "    # run the summary generation workflow\n",
        "    summary = generate_summary(video_transcript)\n",
        "\n",
        "    # save summary to file\n",
        "    f = open(summary_file, 'w')\n",
        "    f.write(summary)\n",
        "    f.close()\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 3: Create embeddings of the transcript summary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import openai\n",
        "import csv\n",
        "\n",
        "openai.api_key = OPENAI_API_KEY\n",
        "\n",
        "def create_embeddings(data_file, video_id):\n",
        "  with open(data_file, 'w') as csv_file:\n",
        "    csv_writer = csv.writer(csv_file)\n",
        "\n",
        "    # write the header\n",
        "    csv_writer.writerow(['video_id', 'text', 'embedding'])\n",
        "\n",
        "    # write the data rows\n",
        "    with open('./summaries/openai-' + video_id + '.txt') as text_file:\n",
        "      summary = text_file.read()\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size = 2048,\n",
        "        chunk_overlap  = 20,\n",
        "      )\n",
        "      documents = text_splitter.create_documents([summary])\n",
        "\n",
        "      for text in documents:\n",
        "        embedding=openai.Embedding.create(model=\"text-embedding-ada-002\", input=text.page_content)\n",
        "        query_result=embedding['data'][0]['embedding']\n",
        "        csv_writer.writerow([video_id, text.page_content, query_result])\n",
        "\n",
        "\n",
        "for video in videos:\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  data_file = './data/' + video_id + '.csv'\n",
        "  if not os.path.exists(data_file):\n",
        "    print(f'Generating embeddings for video id {video_id}')\n",
        "    create_embeddings(data_file, video_id) "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Step 4: Clustering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# TODO: use the csv data to generate the clusters\n",
        "\n",
        "embeddings = np.empty((0, 1536), float) # todo: change hardcoded number\n",
        "embeddings_df = pd.DataFrame()\n",
        "\n",
        "for video in videos:\n",
        "  video_id = video['link'].split('=')[1]\n",
        "  datafile_path = './data/' + video_id + '.csv'\n",
        "  df = pd.read_csv(datafile_path)\n",
        "  df[\"embedding\"] = df.embedding.apply(eval).apply(np.array)\n",
        "  matrix = np.vstack(df.embedding.values)\n",
        "\n",
        "  # append the matrix and df to the embeddings\n",
        "  embeddings = np.vstack((embeddings, matrix))\n",
        "  embeddings_df = embeddings_df.append(df)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "n_clusters = 4\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\", random_state=42)\n",
        "kmeans.fit(embeddings)\n",
        "labels = kmeans.labels_\n",
        "embeddings_df['cluster'] = labels\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "tsne = TSNE(n_components=2, perplexity=3, random_state=42, init=\"random\", learning_rate=200)\n",
        "vis_dims2 = tsne.fit_transform(embeddings)\n",
        "\n",
        "x_coords = [x for x, y in vis_dims2]\n",
        "y_coords = [y for x, y in vis_dims2]\n",
        "\n",
        "for category, color in enumerate([\"purple\", \"green\", \"red\", \"black\"]):\n",
        "  xs = np.array(x_coords)[np.where(embeddings_df['cluster'] == category)[0]]\n",
        "  ys = np.array(y_coords)[np.where(embeddings_df['cluster'] == category)[0]]\n",
        "  # get the video id for this category of points\n",
        "  vid_id = embeddings_df.loc[(embeddings_df['cluster'] == category), 'video_id'].iloc[0] \n",
        "  plt.scatter(xs, ys, color=color, alpha=0.3)\n",
        "\n",
        "  # label points with their video_id\n",
        "  for x,y in zip(xs,ys):\n",
        "    label = vid_id[:2]\n",
        "\n",
        "    plt.annotate(label, (x,y), textcoords=\"offset points\", xytext=(0,10), ha='center', fontsize=8)\n",
        "\n",
        "  # plt.scatter(avg_x, avg_y, marker=\"x\", color=color, s=100)\n",
        "plt.title(\"Clusters identified visualized in 2d using t-SNE\")\n",
        "plt.savefig('images/categories.png')\n",
        "\n",
        "  \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
